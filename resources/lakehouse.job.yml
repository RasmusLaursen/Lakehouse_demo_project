resources:
  jobs:
    lakehouse_demo_project_job:
      name: lakehouse_demo_project_job_${var.environment}

      parameters:
        - name: environment
          default: dev
        - name: dest_catalog
          default: rahl_playground  
        - name: lakehouse_landing_schema
          default: ${resources.schemas.lakehouse_landing_schema.name}
        - name: review_landing_schema
          default: ${resources.schemas.review_landing_schema.name}
        - name: landing_catalog
          default: ${resources.schemas.lakehouse_landing_schema.catalog_name}
        - name: raw_catalog
          default: ${resources.schemas.lakehouse_raw_schema.catalog_name}
        - name: base_catalog
          default: ${resources.schemas.lakehouse_base_schema.catalog_name}
        - name: enriched_catalog
          default: ${resources.schemas.enriched_schema.catalog_name}
        - name: curated_catalog
          default: ${resources.schemas.dimensions_schema.catalog_name}
        - name: curated_metrics_schema
          default: ${resources.schemas.metrics_schema.name}
        - name: curated_dimensions_schema
          default: ${resources.schemas.dimensions_schema.name}
        - name: curated_facts_schema
          default: ${resources.schemas.facts_schema.name} 


      # job_clusters:
      # - job_cluster_key: Job_cluster
      #   new_cluster:
      #     driver_node_type_id: Standard_DS3_v2
      #     node_type_id: Standard_DS3_v2  
      #     spark_version: 15.4.x-scala2.12
      #     is_single_node: true
      #     data_security_mode: SINGLE_USER
      #     kind: CLASSIC_PREVIEW

      # trigger:
      #   # Run this job every day, exactly one day from the last run
      #   # See https://docs.databricks.com/api/workspace/jobs/create#trigger
      #   periodic:
      #     interval: 1
      #     unit: DAYS
      environments:
        - environment_key: default
          spec:
            client: "3"
            dependencies:
              - ../dist/*.whl     
      performance_target: PERFORMANCE_OPTIMIZED
  
      tasks:
        - task_key: generate_synthetic_landing_data
          python_wheel_task:
            package_name: lakehouse_demo_project
            entry_point: source
          environment_key: default   

        # - task_key: refresh_pipeline
        # - task_key: generate_synthetic_landing_data
        #   notebook_task: 
        #     notebook_path: ../src/landing/test_data_generation.py
        #     source: WORKSPACE
        #   job_cluster_key: Job_cluster
        #   libraries:
        #     - pypi:
        #         package: faker == 37.6.0
        # - task_key: refresh_pipeline
        #   depends_on: 
        #     - task_key: generate_synthetic_landing_data        
        #   pipeline_task:
        #     pipeline_id: ${resources.pipelines.pipeline_rahl_lakeflow_connect_sql_demo.id}
        - task_key: lakehouse_model_pipelines
          depends_on: 
            # - task_key: refresh_pipeline
            - task_key: generate_synthetic_landing_data
          pipeline_task:
            pipeline_id: ${resources.pipelines.lakehouse_demo_project_pipeline.id}
        - task_key: lakehouse_metric
          depends_on: 
            - task_key: lakehouse_model_pipelines
          python_wheel_task:
            package_name: lakehouse_demo_project
            entry_point: metric_view
          environment_key: default
        - task_key: Refresh_Lakhouse_Dashboard
          depends_on: 
            - task_key: lakehouse_metric
          dashboard_task:
            subscription: {}
            warehouse_id: ${var.warehouse_id}
            dashboard_id: ${resources.dashboards.lakehouse_demo_project_dashboard.id}
# ${resources.dashboards.nyc_taxi_trip_analysis.id}
# The main job for lakehouse.

# resources:
#   jobs:
#     lakehouse_job:
#       name: lakehouse_job

#       trigger:
#         # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
#         periodic:
#           interval: 1
#           unit: DAYS

#       #email_notifications:
#       #  on_failure:
#       #    - your_email@example.com

#       tasks:

#         - task_key: refresh_pipeline
#           pipeline_task:
#             pipeline_id: ${resources.pipelines.lakehouse_pipeline.id}

#         - task_key: main_task
#           depends_on:
#             - task_key: refresh_pipeline
#           environment_key: default
#           python_wheel_task:
#             package_name: lakehouse
#             entry_point: main

#       # A list of task execution environment specifications that can be referenced by tasks of this job.
#       environments:
#         - environment_key: default

#           # Full documentation of this spec can be found at:
#           # https://docs.databricks.com/api/workspace/jobs/create#environments-spec
#           spec:
#             client: "2"
#             dependencies:
#               - ../dist/*.whl